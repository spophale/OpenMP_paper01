As we move toward Exascale, some of the OpenPOWER compute nodes for these systems 
are becoming more powerful and complex to program. In the next generation systems will have massive amounts of parallelisms where threads 
may be running on CPU cores as well as on accelerators. Advances in memory interconnects, such as NVLINK, will provide a 
shared memory address spaces for different types of memories including high bandwidth memory (HBM), DRAM, etc. 
In preparation for such systems, we need to work more on our understanding of current in-node programming models like OpenMP supports
the concept of affinity as well as memory placement for massive thread-based systems. Data locality and affinity are key
concepts to exploit locality and the memory capabilities for these next-generation of systems. 
In this paper, we evaluate the latest affinity features of OpenMP 4.5 using Oak Ridge National Laboratory mini-apps 
and test them on a system with two Power8 and K40 NVIDIAs. We experiment with the different affinity settings provided by OpenMP 4.5 and measure their effects via hardware counters available on the POWER8 system to measure affinity at scale. Based on this study we describe the current state of art, the challenges we faced in quantifying effects of affinity, inadequacy in the current OpenMP programming model and how some of these may be addressed in OpenMP 5.0.